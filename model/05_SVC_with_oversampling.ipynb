{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Boss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Boss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "#matplotlib imports are used to plot confusion matrices for the classifiers\n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Process accented characters\n",
    "import unicodedata\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import different metrics to evaluate the classifiers\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#import time function from time module to track the training duration\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Alchemy\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine and connection\n",
    "engine = create_engine(\"sqlite:///../db.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuisine_ingredients']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for tables\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv file\n",
    "df = pd.read_sql_query('SELECT * FROM cuisine_ingredients', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>recipe</th>\n",
       "      <th>full_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>3728</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Spicy fregola with scallops and crispy kale</td>\n",
       "      <td>['olive oil, for shallow and deep frying', '1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>1924</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Chunky bacon and cucumber salad</td>\n",
       "      <td>['1 tbsp groundnut oil', '6 long dried chillie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3552</th>\n",
       "      <td>3553</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Polenta pork</td>\n",
       "      <td>['2 tbsp sunflower oil ', '12 rashers thick ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1147</td>\n",
       "      <td>British</td>\n",
       "      <td>Oat fig stuffing</td>\n",
       "      <td>['140g/5oz butter', '100g/3½oz jumbo oats', '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>3196</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Bolognese with tagliatelle</td>\n",
       "      <td>['250g/9oz 00 flour', '3-4 medium free-range e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>1644</td>\n",
       "      <td>British</td>\n",
       "      <td>Textured potato salad</td>\n",
       "      <td>['675g/1½lb floury potatoes, peeled and cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>3055</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Tandoori lamb wrap</td>\n",
       "      <td>['150ml/5fl oz Greek-style yoghurt ', '½ small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>843</td>\n",
       "      <td>British</td>\n",
       "      <td>Gammon and pease pudding</td>\n",
       "      <td>['300g/10oz dried yellow split peas', '50g/2oz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>506</td>\n",
       "      <td>British</td>\n",
       "      <td>Braised shin of beef with parsnip purée</td>\n",
       "      <td>['4kg/9lb beef shin, on the bone', 'sea salt a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>2225</td>\n",
       "      <td>French</td>\n",
       "      <td>Mary Berry's celeriac remoulade</td>\n",
       "      <td>['650g/1lb 7oz celeriac, peeled and sliced int...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cuisine                                       recipe  \\\n",
       "3727  3728  Italian  Spicy fregola with scallops and crispy kale   \n",
       "1923  1924  Chinese              Chunky bacon and cucumber salad   \n",
       "3552  3553  Italian                                Polenta pork    \n",
       "1146  1147  British                             Oat fig stuffing   \n",
       "3195  3196  Italian                   Bolognese with tagliatelle   \n",
       "1643  1644  British                        Textured potato salad   \n",
       "3054  3055   Indian                           Tandoori lamb wrap   \n",
       "842    843  British                     Gammon and pease pudding   \n",
       "505    506  British      Braised shin of beef with parsnip purée   \n",
       "2224  2225   French              Mary Berry's celeriac remoulade   \n",
       "\n",
       "                                       full_ingredients  \n",
       "3727  ['olive oil, for shallow and deep frying', '1 ...  \n",
       "1923  ['1 tbsp groundnut oil', '6 long dried chillie...  \n",
       "3552  ['2 tbsp sunflower oil ', '12 rashers thick ri...  \n",
       "1146  ['140g/5oz butter', '100g/3½oz jumbo oats', '1...  \n",
       "3195  ['250g/9oz 00 flour', '3-4 medium free-range e...  \n",
       "1643  ['675g/1½lb floury potatoes, peeled and cut in...  \n",
       "3054  ['150ml/5fl oz Greek-style yoghurt ', '½ small...  \n",
       "842   ['300g/10oz dried yellow split peas', '50g/2oz...  \n",
       "505   ['4kg/9lb beef shin, on the bone', 'sea salt a...  \n",
       "2224  ['650g/1lb 7oz celeriac, peeled and sliced int...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the data set\n",
    "df.sample(10, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['African',\n",
       " 'American',\n",
       " 'British',\n",
       " 'Caribbean',\n",
       " 'Chinese',\n",
       " 'East European',\n",
       " 'French',\n",
       " 'Greek',\n",
       " 'Indian',\n",
       " 'Irish',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Mexican',\n",
       " 'Nordic',\n",
       " 'North African',\n",
       " 'Pakistani',\n",
       " 'Portuguese',\n",
       " 'South American',\n",
       " 'Spanish',\n",
       " 'Thai and South-East Asian',\n",
       " 'Turkish and Middle Eastern']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of unique cuisines in the dataset\n",
    "cuisine_list = df.cuisine.unique().tolist()\n",
    "cuisine_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define text pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to remove accented characters\n",
    "def remove_accented_chars(matchobj):\n",
    "    text = matchobj.group()\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm combining the text cleaning and pre-processing steps as explored and explained in the 00_EDA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean ingredient text\n",
    "def clean(doc):\n",
    "    doc = doc.str.lower()\n",
    "    doc = doc.str.replace(r'\\w*[\\d¼½¾⅓⅔⅛⅜⅝]\\w*', '')\n",
    "    doc = doc.str.translate(str.maketrans('', '', string.punctuation))\n",
    "    doc = doc.str.replace(r'[£×–‘’“”⁄]', '')\n",
    "#     doc = doc.apply(lambda x: re.sub(r'[âãäçèéêîïñóôûüōưấớ]', remove_accented_chars, x))\n",
    "    doc = doc.apply(lambda x: word_tokenize(x))\n",
    "    doc = doc.apply(lambda x: [word for word in x if not word in stop_words_nltk])\n",
    "    doc = doc.apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    processed_doc = doc.apply(lambda x: ' '.join([word for word in x]))\n",
    "\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the dataframe with the cleaned text\n",
    "df[\"ingredients_processed\"] = clean(df.full_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>recipe</th>\n",
       "      <th>full_ingredients</th>\n",
       "      <th>ingredients_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>3728</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Spicy fregola with scallops and crispy kale</td>\n",
       "      <td>['olive oil, for shallow and deep frying', '1 ...</td>\n",
       "      <td>oliv oil shallow deep fri shallot fine chop ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>1924</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Chunky bacon and cucumber salad</td>\n",
       "      <td>['1 tbsp groundnut oil', '6 long dried chillie...</td>\n",
       "      <td>tbsp groundnut oil long dri chilli tsp sichuan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3552</th>\n",
       "      <td>3553</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Polenta pork</td>\n",
       "      <td>['2 tbsp sunflower oil ', '12 rashers thick ri...</td>\n",
       "      <td>tbsp sunflow oil rasher thick rindless smoke s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1147</td>\n",
       "      <td>British</td>\n",
       "      <td>Oat fig stuffing</td>\n",
       "      <td>['140g/5oz butter', '100g/3½oz jumbo oats', '1...</td>\n",
       "      <td>butter jumbo oat roll oat red onion roughli ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>3196</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Bolognese with tagliatelle</td>\n",
       "      <td>['250g/9oz 00 flour', '3-4 medium free-range e...</td>\n",
       "      <td>flour medium freerang egg prefer bright yellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>1644</td>\n",
       "      <td>British</td>\n",
       "      <td>Textured potato salad</td>\n",
       "      <td>['675g/1½lb floury potatoes, peeled and cut in...</td>\n",
       "      <td>flouri potato peel cut cube rasher smoke strea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>3055</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Tandoori lamb wrap</td>\n",
       "      <td>['150ml/5fl oz Greek-style yoghurt ', '½ small...</td>\n",
       "      <td>oz greekstyl yoghurt small onion roughli chop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>843</td>\n",
       "      <td>British</td>\n",
       "      <td>Gammon and pease pudding</td>\n",
       "      <td>['300g/10oz dried yellow split peas', '50g/2oz...</td>\n",
       "      <td>dri yellow split pea butter onion roughli chop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>506</td>\n",
       "      <td>British</td>\n",
       "      <td>Braised shin of beef with parsnip purée</td>\n",
       "      <td>['4kg/9lb beef shin, on the bone', 'sea salt a...</td>\n",
       "      <td>beef shin bone sea salt freshli ground black p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>2225</td>\n",
       "      <td>French</td>\n",
       "      <td>Mary Berry's celeriac remoulade</td>\n",
       "      <td>['650g/1lb 7oz celeriac, peeled and sliced int...</td>\n",
       "      <td>celeriac peel slice thin matchstick see tip le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  cuisine                                       recipe  \\\n",
       "3727  3728  Italian  Spicy fregola with scallops and crispy kale   \n",
       "1923  1924  Chinese              Chunky bacon and cucumber salad   \n",
       "3552  3553  Italian                                Polenta pork    \n",
       "1146  1147  British                             Oat fig stuffing   \n",
       "3195  3196  Italian                   Bolognese with tagliatelle   \n",
       "1643  1644  British                        Textured potato salad   \n",
       "3054  3055   Indian                           Tandoori lamb wrap   \n",
       "842    843  British                     Gammon and pease pudding   \n",
       "505    506  British      Braised shin of beef with parsnip purée   \n",
       "2224  2225   French              Mary Berry's celeriac remoulade   \n",
       "\n",
       "                                       full_ingredients  \\\n",
       "3727  ['olive oil, for shallow and deep frying', '1 ...   \n",
       "1923  ['1 tbsp groundnut oil', '6 long dried chillie...   \n",
       "3552  ['2 tbsp sunflower oil ', '12 rashers thick ri...   \n",
       "1146  ['140g/5oz butter', '100g/3½oz jumbo oats', '1...   \n",
       "3195  ['250g/9oz 00 flour', '3-4 medium free-range e...   \n",
       "1643  ['675g/1½lb floury potatoes, peeled and cut in...   \n",
       "3054  ['150ml/5fl oz Greek-style yoghurt ', '½ small...   \n",
       "842   ['300g/10oz dried yellow split peas', '50g/2oz...   \n",
       "505   ['4kg/9lb beef shin, on the bone', 'sea salt a...   \n",
       "2224  ['650g/1lb 7oz celeriac, peeled and sliced int...   \n",
       "\n",
       "                                  ingredients_processed  \n",
       "3727  oliv oil shallow deep fri shallot fine chop ga...  \n",
       "1923  tbsp groundnut oil long dri chilli tsp sichuan...  \n",
       "3552  tbsp sunflow oil rasher thick rindless smoke s...  \n",
       "1146  butter jumbo oat roll oat red onion roughli ch...  \n",
       "3195  flour medium freerang egg prefer bright yellow...  \n",
       "1643  flouri potato peel cut cube rasher smoke strea...  \n",
       "3054  oz greekstyl yoghurt small onion roughli chop ...  \n",
       "842   dri yellow split pea butter onion roughli chop...  \n",
       "505   beef shin bone sea salt freshli ground black p...  \n",
       "2224  celeriac peel slice thin matchstick see tip le...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the dataset\n",
    "df.sample(10, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets (75% train and 25% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4724,) (4724,)\n"
     ]
    }
   ],
   "source": [
    "# The column contains textual data to extract features from.\n",
    "X = df.ingredients_processed\n",
    "\n",
    "# The column we're learning to predict.\n",
    "y = df.cuisine\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3543,) (3543,)\n",
      "(1181,) (1181,)\n"
     ]
    }
   ],
   "source": [
    "# Split X and y into training and testing sets. By default, it splits 75% training and 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3543, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate our training data back together\n",
    "X_y_train = pd.concat([X_train, y_train], axis=1)\n",
    "X_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients_processed</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>fresh white fish fillet sole plaic cornflour d...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>tbsp oliv oil goodqual pork sausag shallot fin...</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4215</th>\n",
       "      <td>tbsp veget oil onion fine chop garlic clove fi...</td>\n",
       "      <td>Pakistani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>x sirloin steak salt freshli ground black pepp...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>type pasta flour avail supermarket bake sectio...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ingredients_processed    cuisine\n",
       "1948  fresh white fish fillet sole plaic cornflour d...    Chinese\n",
       "921   tbsp oliv oil goodqual pork sausag shallot fin...    British\n",
       "4215  tbsp veget oil onion fine chop garlic clove fi...  Pakistani\n",
       "2601  x sirloin steak salt freshli ground black pepp...     French\n",
       "3770  type pasta flour avail supermarket bake sectio...    Italian"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the training set\n",
    "X_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients_processed</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>fresh white fish fillet sole plaic cornflour d...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4215</th>\n",
       "      <td>tbsp veget oil onion fine chop garlic clove fi...</td>\n",
       "      <td>Pakistani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>x sirloin steak salt freshli ground black pepp...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>type pasta flour avail supermarket bake sectio...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>tagliatel broccoli cut small floret hand pea f...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ingredients_processed    cuisine\n",
       "1948  fresh white fish fillet sole plaic cornflour d...    Chinese\n",
       "4215  tbsp veget oil onion fine chop garlic clove fi...  Pakistani\n",
       "2601  x sirloin steak salt freshli ground black pepp...     French\n",
       "3770  type pasta flour avail supermarket bake sectio...    Italian\n",
       "3737  tagliatel broccoli cut small floret hand pea f...    Italian"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate minority and majority classes\n",
    "british_cuisines_df = X_y_train[X_y_train.cuisine==\"British\"]\n",
    "other_cuisines_df = X_y_train[X_y_train.cuisine!=\"British\"]\n",
    "other_cuisines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chinese',\n",
       " 'Pakistani',\n",
       " 'French',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Indian',\n",
       " 'Mexican',\n",
       " 'American',\n",
       " 'Nordic',\n",
       " 'Turkish and Middle Eastern',\n",
       " 'Caribbean',\n",
       " 'Thai and South-East Asian',\n",
       " 'North African',\n",
       " 'East European',\n",
       " 'Irish',\n",
       " 'Portuguese',\n",
       " 'Greek',\n",
       " 'South American',\n",
       " 'Spanish',\n",
       " 'Korean',\n",
       " 'African']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of minority cuisines\n",
    "other_cuisines = other_cuisines_df.cuisine.unique().tolist()\n",
    "other_cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cuisines_upsampled = list()\n",
    "\n",
    "# Upsample the minorities\n",
    "\n",
    "for cuisine in other_cuisines:\n",
    "    cuisine_df = X_y_train[X_y_train.cuisine==cuisine]\n",
    "    cuisine_upsampled = resample(cuisine_df,\n",
    "                                 replace=True, # sample with replacement\n",
    "                                 n_samples=len(british_cuisines_df), # match number of recipes in British cuisine\n",
    "                                 random_state=1)\n",
    "    other_cuisines_upsampled.append(cuisine_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients_processed</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>tbsp veget oil plu extra shallow fri firm tofu...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>firm white fish fillet cod sole skin whole fis...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>tbsp shaox rice wine dri sherri tbsp light soy...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>x lobster tsp sesam oil fresh root ginger peel...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>pork minc ginger grate garlic grate spring oni...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ingredients_processed  cuisine\n",
       "1909  tbsp veget oil plu extra shallow fri firm tofu...  Chinese\n",
       "2041  firm white fish fillet cod sole skin whole fis...  Chinese\n",
       "2019  tbsp shaox rice wine dri sherri tbsp light soy...  Chinese\n",
       "1980  x lobster tsp sesam oil fresh root ginger peel...  Chinese\n",
       "1999  pork minc ginger grate garlic grate spring oni...  Chinese"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new resampled data set for minority cuisines\n",
    "other_cuisines_upsampled = pd.concat(other_cuisines_upsampled)\n",
    "other_cuisines_upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the majority and the upsampled minority\n",
    "upsampled = pd.concat([british_cuisines_df, other_cuisines_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Japanese                      1098\n",
       "British                       1098\n",
       "Pakistani                     1098\n",
       "Irish                         1098\n",
       "French                        1098\n",
       "American                      1098\n",
       "Chinese                       1098\n",
       "Mexican                       1098\n",
       "South American                1098\n",
       "Caribbean                     1098\n",
       "Portuguese                    1098\n",
       "Greek                         1098\n",
       "Turkish and Middle Eastern    1098\n",
       "Spanish                       1098\n",
       "North African                 1098\n",
       "Thai and South-East Asian     1098\n",
       "Korean                        1098\n",
       "East European                 1098\n",
       "Nordic                        1098\n",
       "African                       1098\n",
       "Indian                        1098\n",
       "Italian                       1098\n",
       "Name: cuisine, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check new class counts\n",
    "upsampled.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients_processed</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>oliv oil fri larg chicken thigh onion fine cho...</td>\n",
       "      <td>North African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>flour freerang italian yellow golden egg durum...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>salt butter room temperatur caster sugar star ...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4159</th>\n",
       "      <td>small chicken joint piec tbsp oliv oil preserv...</td>\n",
       "      <td>North African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>tbsp ghee clarifi butter onion fine chop fresh...</td>\n",
       "      <td>Pakistani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>fillet beef garlic clove fresh root ginger sli...</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>whole king prawn around medium oz oliv oil gar...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>tbsp oliv oil plu extra greas beef minc red on...</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>salmon fillet skin longgrain rice butter plu e...</td>\n",
       "      <td>East European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>duck breast skin oz veget oil oz dark soy sauc...</td>\n",
       "      <td>Thai and South-East Asian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ingredients_processed  \\\n",
       "4133  oliv oil fri larg chicken thigh onion fine cho...   \n",
       "3773  flour freerang italian yellow golden egg durum...   \n",
       "2592  salt butter room temperatur caster sugar star ...   \n",
       "4159  small chicken joint piec tbsp oliv oil preserv...   \n",
       "4204  tbsp ghee clarifi butter onion fine chop fresh...   \n",
       "3943  fillet beef garlic clove fresh root ginger sli...   \n",
       "4287  whole king prawn around medium oz oliv oil gar...   \n",
       "4046  tbsp oliv oil plu extra greas beef minc red on...   \n",
       "2145  salmon fillet skin longgrain rice butter plu e...   \n",
       "4413  duck breast skin oz veget oil oz dark soy sauc...   \n",
       "\n",
       "                        cuisine  \n",
       "4133              North African  \n",
       "3773                    Italian  \n",
       "2592                     French  \n",
       "4159              North African  \n",
       "4204                  Pakistani  \n",
       "3943                     Korean  \n",
       "4287                    Spanish  \n",
       "4046                    Mexican  \n",
       "2145              East European  \n",
       "4413  Thai and South-East Asian  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the upsampled dataset\n",
    "upsampled.sample(10, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bags of words and bags of n-grams approaches, all words in a corpus are treated equally important. TF-IDF, meanwhile, emphasizes that some words in a document are more important than others. For the current classification problem, I find that TF-IDF would suit the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = upsampled.ingredients_processed\n",
    "y_train_new = upsampled.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7a09b40cff58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m tfidf_count_occur_df = pd.DataFrame(\n\u001b[0;32m      4\u001b[0m     (count, word) for word, count in zip(\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtfidf_count_occurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     tfidf_vec.get_feature_names()))\n\u001b[0;32m      7\u001b[0m \u001b[0mtfidf_count_occur_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_count_occurs = tfidf_vec.fit_transform(X_train_new)\n",
    "tfidf_count_occur_df = pd.DataFrame(\n",
    "    (count, word) for word, count in zip(\n",
    "    tfidf_count_occurs.toarray().tolist()[0],   \n",
    "    tfidf_vec.get_feature_names()))\n",
    "tfidf_count_occur_df.columns = ['Word', 'Count']\n",
    "tfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "tfidf_count_occur_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = upsampled.ingredients_processed\n",
    "y_train_new = upsampled.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize train and test data\n",
    "X_train_transformed = tfidf.fit_transform(X_train_new)\n",
    "print(X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine linear classifier\n",
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier and time the training step\n",
    "%time model.fit(X_train_transformed, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class predictions for X_test_transformed\n",
    "X_test_transformed = tfidf.transform(X_test)\n",
    "y_predicted = model.predict(X_test_transformed)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluate the model using accuracy, confusion matrix, and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Dummy Classifer to classify cuisines without even training a \"real\" model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy = DummyClassifier().fit(X_train_transformed, y_train_new)\n",
    "dummy_pred = dummy.predict(X_test_transformed)\n",
    "\n",
    "# Check accuracy on test set\n",
    "print('Dummy Classifer Test Accuracy: ', accuracy_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on test set:\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print accuracy on train set:\n",
    "print(\"Train Accuracy: \", model.score(X_train_transformed, y_train_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Accuracy is not the best metric to evaluate imbalanced testing datasets as it can be very misleading. I'll use confusion matrix and classification report for further understanding about the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix. \n",
    "# Ref: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_predicted)\n",
    "plt.figure(figsize=(16, 12))\n",
    "plot_confusion_matrix(cnf_matrix, classes=cuisine_list, normalize=True,\n",
    "                      title='Confusion matrix with all features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can notice that the classifier is doing poorly with identifying xxx, while it is doing well with xxx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predicted,\n",
    "                            target_names=cuisine_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we choose whats the best? If we look at overall accuracy alone, we should be choosing the very first classifier in this notebook. However, that is also doing poorly with identifying \"relevant\" articles. If we choose purely based on how good it is doing with \"relevant\" category, we should choose the second one we built. If we choose purely based on how good it is doing with \"irrelevant\" category, surely, nothing beats not building any classifier and just calling everything irrelevant! So, what to choose as the best among these depends on what we are looking for in our usecase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column contains textual data to extract features from.\n",
    "X = df.ingredients_processed\n",
    "X_transformed = new_tfidf.fit_transform(X)\n",
    "\n",
    "# The column we're learning to predict.\n",
    "y = df.cuisine \n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 folds\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate multiple models using kfolds\n",
    "results = cross_val_score(model, X_transformed, y, cv=kfold)\n",
    "print(results)\n",
    "print(\"Accuracy Score Mean:\", results.mean())\n",
    "print(\"Accuracy Score Std:\", results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GridSearchCV to tune the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'kernel': [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n",
    "# grid = GridSearchCV(model, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid2.best_params_)\n",
    "# print(grid2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"butter, for greasing \\\n",
    "        400ml/14fl oz full-fat milk \\\n",
    "        50g/1¾oz fresh white breadcrumbs \\\n",
    "        4 large free-range eggs, beaten \\\n",
    "        1 tbsp olive oil \\\n",
    "        1kg/2lb 4oz lamb mince \\\n",
    "        2 onions, finely chopped \\\n",
    "        1 bay leaf \\\n",
    "        2 garlic cloves, crushed \\\n",
    "        2 tbsp madras curry paste \\\n",
    "        250ml/9fl oz lamb stock \\\n",
    "        2 tbsp Worcestershire sauce \\\n",
    "        2 tbsp mango chutney \\\n",
    "        50g/1¾oz raisins \\\n",
    "        50g/1¾oz dried apricots, chopped \\\n",
    "        1 tbsp cider vinegar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\w*[\\d¼½¾⅓⅔⅛⅜⅝]\\w*', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[£×–‘’“”⁄]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words_no_stopwords = [word for word in words if not word in stop_words_nltk]\n",
    "    stemmed_words = [stemmer.stem(word) for word in words_no_stopwords]\n",
    "    processed_text = ' '.join([word for word in stemmed_words])\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned = pd.Series([preprocess(text)])\n",
    "X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(tfidf.transform(X_cleaned))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonAdv] *",
   "language": "python",
   "name": "conda-env-PythonAdv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
